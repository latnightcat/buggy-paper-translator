title,authors,abstract,date,paper_url,score,title_cn,abstract_cn
Meta-Unlearning on Diffusion Models: Preventing Relearning Unlearned Concepts,"Hongcheng Gao, Tianyu Pang, Chao Du, Taihang Hu, Zhijie Deng, Min Lin","With the rapid progress of diffusion models (DMs), significant efforts are being made to unlearn harmful or copyrighted concepts from pretrained DMs to prevent potential model misuse. However, it is observed that even when DMs are properly unlearned before release, malicious finetuning can compromise this process, causing DMs to relearn the unlearned concepts. This occurs partly because certain benign concepts (e.g., ""skin"") retained in DMs are related to the unlearned ones (e.g., ""nudity""), facilitating their relearning via finetuning. To address this, we propose meta-unlearning on DMs. Intuitively, a meta-unlearned DM should behave like an unlearned DM when used as is; moreover, if the meta-unlearned DM undergoes malicious finetuning on unlearned concepts, the related benign concepts retained within it will be triggered to self-destruct, hindering the relearning of unlearned concepts. Our meta-unlearning framework is compatible with most existing unlearning methods, requiring only the addition of an easy-to-implement meta objective. We validate our approach through empirical experiments on meta-unlearning concepts from Stable Diffusion models (SD-v1-4 and SDXL), supported by extensive ablation studies.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Gao_Meta-Unlearning_on_Diffusion_Models_Preventing_Relearning_Unlearned_Concepts_ICCV_2025_paper.html,0.99609375,元遗忘在扩散模型中的应用：防止已遗忘概念的再学习,随着扩散模型的快速发展，研究者们正致力于从预训练模型中消除有害或受版权保护的概念，以防止模型被滥用。然而研究发现，即使扩散模型在发布前已进行适当的遗忘处理，恶意的微调仍可能破坏这一过程，导致模型重新习得已被遗忘的概念。这种现象部分源于扩散模型中保留的某些良性概念（如'皮肤'）与已遗忘概念（如'裸露'）存在关联，通过微调会促进这些概念的重新习得。为解决此问题，我们提出对扩散模型进行元遗忘处理。直观而言，经过元遗忘处理的扩散模型在直接使用时应与普通遗忘模型表现一致；更重要的是，若该模型在已遗忘概念上遭受恶意微调，其内部保留的相关良性概念将被触发自我销毁机制，从而阻碍已遗忘概念的重新习得。我们的元遗忘框架兼容现有大多数遗忘方法，仅需添加易于实现的元目标函数即可。通过在Stable Diffusion模型（SD-v1-4和SDXL）上进行元遗忘概念的实证实验，并结合大量消融研究，我们验证了该方法的有效性。
Your Text Encoder Can Be An Object-Level Watermarking Controller,"Naresh Kumar Devulapally, Mingzhen Huang, Vishal Asnani, Shruti Agarwal, Siwei Lyu, Vishnu Suresh Lokhande","Invisible watermarking of AI-generated images can help with copyright protection, enabling detection and identification of AI-generated media. In this work, we present a novel approach to watermark images of T2I Latent Diffusion Models (LDMs). By only fine-tuning text token embeddings \mathcal W _*, we enable watermarking in selected objects or parts of the image, offering greater flexibility compared to traditional full-image watermarking. Our method leverages the text encoder's compatibility across various LDMs, allowing plug-and-play integration for different LDMs. Moreover, introducing the watermark early in the encoding stage improves robustness to adversarial perturbations in later stages of the pipeline. Our approach achieves 99% bit accuracy (48 bits) with a 10^5 xreduction in model parameters, enabling efficient watermarking.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Devulapally_Your_Text_Encoder_Can_Be_An_Object-Level_Watermarking_Controller_ICCV_2025_paper.html,0.96875,您的文本编码器可作为对象级水印控制器,AI生成图像的不可见水印技术有助于版权保护，使得AI生成媒体的检测与识别成为可能。本研究提出了一种针对文本到图像潜在扩散模型（LDMs）生成图像的新型水印方法。通过仅微调文本标记嵌入向量\mathcal W _*，我们实现了对图像中特定对象或局部区域的水印嵌入，相比传统的全图像水印技术具有更高的灵活性。该方法利用文本编码器在不同LDMs间的兼容性，可即插即用地集成于各类LDM架构。此外，在编码阶段早期引入水印能有效提升对后续流程中对抗性扰动的鲁棒性。我们的方法以10^5倍的参数量缩减实现了99%的比特准确率（48位），为高效水印嵌入提供了可行方案。
Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning,"Saemi Moon, Minjong Lee, Sangdon Park, Dongwoo Kim","As text-to-image diffusion models gain widespread commercial applications, there are increasing concerns about unethical or harmful use, including the unauthorized generation of copyrighted or sensitive content. Concept unlearning has emerged as a promising solution to these challenges by removing undesired and harmful information from the pre-trained model. However, the previous evaluations primarily focus on whether target concepts are removed while preserving image quality, neglecting the broader impacts such as unintended side effects. In this work, we propose Holistic Unlearning Benchmark (HUB), a comprehensive framework for evaluating unlearning methods across six key dimensions: faithfulness, alignment, pinpoint-ness, multilingual robustness, attack robustness, and efficiency. Our benchmark covers 33 target concepts, including 16,000 prompts per concept, spanning four categories: Celebrity, Style, Intellectual Property, and NSFW. Our investigation reveals that no single method excels across all evaluation criteria. By releasing our evaluation code and dataset, we hope to inspire further research in this area, leading to more reliable and effective unlearning methods.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Moon_Holistic_Unlearning_Benchmark_A_Multi-Faceted_Evaluation_for_Text-to-Image_Diffusion_Model_ICCV_2025_paper.html,0.9609375,全面遗忘基准：面向文本到图像扩散模型遗忘的多维度评估,随着文本到图像扩散模型在商业应用中的日益普及，人们对其可能涉及的伦理问题或有害使用（包括未经授权生成受版权保护或敏感内容）的担忧也在增加。概念遗忘作为一种有前景的解决方案应运而生，旨在从预训练模型中移除不良及有害信息。然而，既往评估主要关注目标概念是否被有效移除同时保持图像质量，却忽视了更广泛的影响，如非预期的副作用。本研究提出**整体遗忘基准（HUB）**，这是一个全面的评估框架，从六个关键维度对遗忘方法进行系统评估：忠实度、对齐性、精准性、多语言鲁棒性、攻击鲁棒性以及效率。我们的基准涵盖33个目标概念，每个概念包含16，000条提示词，涵盖四大类别：名人、风格、知识产权及NSFW内容。研究发现，目前尚无单一方法能在所有评估标准上表现优异。通过公开评估代码与数据集，我们希望激发该领域的进一步研究，推动开发更可靠、更有效的遗忘方法。
MUNBa: Machine Unlearning via Nash Bargaining,"Jing Wu, Mehrtash Harandi","Machine Unlearning (MU) aims to selectively erase harmful behaviors from models while retaining the overall utility of the model. As a multi-task learning problem, MU involves balancing objectives related to forgetting specific concepts/data and preserving general performance. A naive integration of these forgetting and preserving objectives can lead to gradient conflicts and dominance, impeding MU algorithms from reaching optimal solutions.To address the gradient conflict and dominance issue, we reformulate MU as a two-player cooperative game, where the two players, namely, the forgetting player and the preservation player, contribute via their gradient proposals to maximize their overall gain and balance their contributions.To this end, inspired by the Nash bargaining theory, we derive a closed-form solution to guide the model toward the Pareto stationary point.Our formulation of MU guarantees an equilibrium solution, where any deviation from the final state would lead to a reduction in the overall objectives for both players, ensuring optimality in each objective.We evaluate our algorithm's effectiveness on a diverse set of tasks across image classification and image generation.Extensive experiments with ResNet, vision-language model CLIP, and text-to-image diffusion models demonstrate that our method outperforms state-of-the-art MU algorithms, achieving a better trade-off between forgetting and preserving.Our results also highlight improvements in forgetting precision, preservation of generalization, and robustness against adversarial attacks.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wu_MUNBa_Machine_Unlearning_via_Nash_Bargaining_ICCV_2025_paper.html,0.94140625,MUNBa：基于纳什议价机制的机器遗忘方法,机器遗忘旨在选择性消除模型中的有害行为，同时保持模型的整体效用。作为一个多任务学习问题，机器遗忘需要平衡遗忘特定概念/数据与保持通用性能这两类目标。若简单整合遗忘与保持目标，可能导致梯度冲突与主导问题，阻碍机器遗忘算法达到最优解。  为解决梯度冲突与主导问题，我们将机器遗忘重构为双玩家合作博弈模型：遗忘玩家与保持玩家通过各自的梯度提案共同贡献，以最大化整体收益并平衡双方贡献。受纳什议价理论启发，我们推导出闭式解以引导模型趋向帕累托稳态点。该重构方案保证了均衡解的存在――任何偏离最终状态的行为都将导致双方整体目标的减损，从而确保各目标的最优性。  我们在图像分类与图像生成等多样化任务中评估算法的有效性。通过对ResNet、视觉语言模型CLIP及文生图扩散模型的大量实验表明，本方法优于当前最先进的机器遗忘算法，在遗忘与保持之间实现了更优权衡。实验结果同时显示，该方法在遗忘精度、泛化能力保持及对抗攻击鲁棒性方面均有显著提升。
Safeguarding Vision-Language Models: Mitigating Vulnerabilities to Gaussian Noise in Perturbation-based Attacks,"Jiawei Wang, Yushen Zuo, Yuanjun Chai, Zhendong Liu, Yicheng Fu, Yichun Feng, Kin-Man Lam","Vision-Language Models (VLMs) extend the capabilities of Large Language Models (LLMs) by incorporating visual information, yet they remain vulnerable to jailbreak attacks, especially when processing noisy or corrupted images. Although existing VLMs adopt security measures during training to mitigate such attacks, vulnerabilities associated with noise-augmented visual inputs are overlooked. In this work, we identify that missing noise-augmented training causes critical security gaps: many VLMs are susceptible to even simple perturbations such as Gaussian noise. To address this challenge, we propose Robust-VLGuard, a multimodal safety dataset with aligned / misaligned image-text pairs, combined with noise-augmented fine-tuning that reduces attack success rates while preserving functionality of VLM. For stronger optimization-based visual perturbation attacks, we propose DiffPure-VLM, leveraging diffusion models to convert adversarial perturbations into Gaussian-like noise, which can be defended by VLMs with noise-augmented safety fine-tuning. Experimental results demonstrate that the distribution-shifting property of diffusion model aligns well with our fine-tuned VLMs, significantly mitigating adversarial perturbations across varying intensities. The dataset and code are available at https://github.com/JarvisUSTC/DiffPure-RobustVLM.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wang_Safeguarding_Vision-Language_Models_Mitigating_Vulnerabilities_to_Gaussian_Noise_in_Perturbation-based_ICCV_2025_paper.html,0.93359375,保护视觉-语言模型：减轻基于扰动攻击中高斯噪声的脆弱性,视觉语言模型（VLMs）通过整合视觉信息扩展了大语言模型（LLMs）的能力，但在处理含噪或受损图像时仍易受越狱攻击。尽管现有VLMs在训练阶段已采用安全措施以缓解此类攻击，但与噪声增强视觉输入相关的漏洞仍被忽视。本研究指出，缺乏噪声增强训练会导致关键安全漏洞：许多VLM甚至对高斯噪声等简单扰动都表现出脆弱性。为应对这一挑战，我们提出Robust-VLGuard――一个包含对齐/非对齐图文对的多模态安全数据集，结合噪声增强微调技术，在保持VLM功能的同时显著降低攻击成功率。针对更强的基于优化的视觉扰动攻击，我们提出DiffPure-VLM方法，利用扩散模型将对抗性扰动转换为类高斯噪声，使经过噪声增强安全微调的VLMs能够有效防御。实验结果表明，扩散模型的分布偏移特性与我们微调后的VLMs高度契合，能显著缓解不同强度的对抗性扰动。数据集与代码已公开于https://github.com/JarvisUSTC/DiffPure-RobustVLM。
Sculpting Memory: Multi-Concept Forgetting in Diffusion Models via Dynamic Mask and Concept-Aware Optimization,"Gen Li, Yang Xiao, Jie Ji, Kaiyuan Deng, Bo Hui, Linke Guo, Xiaolong Ma","Text-to-image (T2I) diffusion models have achieved remarkable success in generating high-quality images from textual prompts. However, their ability to store vast amounts of knowledge raises concerns in scenarios where selective forgetting is necessary, such as removing copyrighted content, reducing biases, or eliminating harmful concepts. While existing unlearning methods can remove certain concepts, they struggle with multi-concept forgetting due to instability, residual knowledge persistence, and generation quality degradation. To address these challenges, we propose **Dynamic Mask coupled with Concept-Aware Loss**, a novel unlearning framework designed for multi-concept forgetting in diffusion models. Our **Dynamic Mask** mechanism adaptively updates gradient masks based on current optimization states, allowing selective weight modifications that prevent interference with unrelated knowledge. Additionally, our **Concept-Aware Loss** explicitly guides the unlearning process by enforcing semantic consistency through superclass alignment, while a regularization loss based on knowledge distillation ensures that previously unlearned concepts remain forgotten during sequential unlearning. We conduct extensive experiments to evaluate our approach. Results demonstrate that our method outperforms existing unlearning techniques in forgetting effectiveness, output fidelity, and semantic coherence, particularly in multi-concept scenarios. Our work provides a principled and flexible framework for stable and high-fidelity unlearning in generative models. Code available in https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Li_Sculpting_Memory_Multi-Concept_Forgetting_in_Diffusion_Models_via_Dynamic_Mask_ICCV_2025_paper.html,0.92578125,塑造记忆：通过动态掩码与概念感知优化实现扩散模型中的多概念遗忘,文本到图像（T2I）扩散模型在根据文本提示生成高质量图像方面取得了显著成功。然而，其存储海量知识的能力在需要选择性遗忘的场景中引发了担忧，例如移除受版权保护的内容、减少偏见或消除有害概念。尽管现有的遗忘方法能够移除某些概念，但由于不稳定性、残留知识持续存在以及生成质量下降等问题，它们在多概念遗忘任务中面临困难。为解决这些挑战，我们提出了**动态掩码与概念感知损失相结合**的新型遗忘框架，专为扩散模型中的多概念遗忘而设计。我们的**动态掩码**机制根据当前优化状态自适应更新梯度掩码，实现选择性权重修改，从而避免对无关知识的干扰。此外，通过超类对齐强制语义一致性，我们的**概念感知损失**显式引导遗忘过程，同时基于知识蒸馏的正则化损失确保在顺序遗忘过程中已遗忘的概念保持遗忘状态。我们进行了大量实验以评估所提方法。结果表明，在遗忘效果、输出保真度和语义连贯性方面，尤其在多概念场景中，我们的方法优于现有遗忘技术。本研究为生成模型中的稳定高保真遗忘提供了一个原则性且灵活的框架。代码发布于 https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025
PlugMark: A Plug-in Zero-Watermarking Framework for Diffusion Models,"Pengzhen Chen, Yanwei Liu, Xiaoyan Gu, Enci Liu, Zhuoyi Shang, Xiangyang Ji, Wu Liu","Diffusion models have significantly advanced the field of image synthesis, making the protection of their intellectual property (IP) a critical concern. Existing IP protection methods primarily focus on embedding watermarks into generated images by altering the structure of the diffusion process. However, these approaches inevitably compromise the quality of the generated images and are particularly vulnerable to fine-tuning attacks, especially for open-source models such as Stable Diffusion (SD). In this paper, we propose PlugMark, a novel plug-in zero-watermarking framework for diffusion models. The core idea of PlugMark is based on two observations: a classifier can be uniquely characterized by its decision boundaries, and a diffusion model can be uniquely represented by the knowledge acquired from training data.Building on this foundation, we introduce a diffusion knowledge extractor that can be plugged into a diffusion model to extract its knowledge and output a classification result. PlugMark subsequently generates boundary representations based on this classification result, serving as a zero-distortion watermark that uniquely represents the decision boundaries and, by extension, the knowledge of the diffusion model. Since only the extractor requires training, the performance of the original diffusion model remains unaffected.Extensive experimental results demonstrate that PlugMark can robustly extract high-confidence zero-watermarks from both the original model and its post-processed versions while effectively distinguishing them from non-post-processed diffusion models.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chen_PlugMark_A_Plug-in_Zero-Watermarking_Framework_for_Diffusion_Models_ICCV_2025_paper.html,0.92578125,PlugMark：一种用于扩散模型的插件式零水印框架,扩散模型显著推动了图像合成领域的发展，使其知识产权保护成为关键议题。现有知识产权保护方法主要通过改变扩散过程的结构，将水印嵌入生成图像中。然而，这些方法不可避免地会损害生成图像的质量，并且特别容易受到微调攻击，对于像稳定扩散这样的开源模型尤其如此。本文提出PlugMark，一种新颖的插件式零水印框架，用于保护扩散模型的知识产权。PlugMark的核心思想基于两个观察：分类器可以通过其决策边界进行唯一表征，而扩散模型则可以通过从训练数据中获取的知识进行唯一表示。在此基础上，我们引入一种扩散知识提取器，可插入扩散模型中提取其知识并输出分类结果。PlugMark随后基于该分类结果生成边界表示，作为零失真水印，唯一地表征决策边界，进而表征扩散模型的知识。由于仅需训练提取器，原始扩散模型的性能不受影响。大量实验结果表明，PlugMark能够从原始模型及其后处理版本中稳健地提取高置信度的零水印，同时有效区分非后处理的扩散模型。
ZIUM: Zero-Shot Intent-Aware Adversarial Attack on Unlearned Models,"Hyun Jun Yook, Ga San Jhun, Jae Hyun Cho, Min Jeon, Donghyun Kim, Tae Hyung Kim, Youn Kyu Lee","Machine unlearning (MU) removes specific data points or concepts from deep learning models to enhance privacy and prevent sensitive content generation. Adversarial prompts can exploit unlearned models to generate content containing removed concepts, posing a significant security risk. However, existing adversarial attack methods still face challenges in generating content that aligns with an attacker's intent while incurring high computational costs to identify successful prompts. To address these challenges, we propose ZIUM, a Zero-shot Intent-aware adversarial attack on Unlearned Models, which enables the flexible customization of target attack images to reflect an attacker's intent. Additionally, ZIUM supports zero-shot adversarial attacks without requiring further optimization for previously attacked unlearned concepts. The evaluation across various MU scenarios demonstrated ZIUM's effectiveness in successfully customizing content based on user-intent prompts while achieving a superior attack success rate compared to existing methods. Moreover, its zero-shot adversarial attack significantly reduces the attack time for previously attacked unlearned concepts.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Yook_ZIUM_Zero-Shot_Intent-Aware_Adversarial_Attack_on_Unlearned_Models_ICCV_2025_paper.html,0.92578125,ZIUM：针对未学习模型的零样本意图感知对抗攻击,机器遗忘（MU）通过从深度学习模型中移除特定数据点或概念，以增强隐私保护并防止生成敏感内容。然而，对抗性提示可利用已遗忘模型生成包含被移除概念的内容，构成重大安全风险。现有对抗攻击方法在生成符合攻击者意图的内容方面仍面临挑战，且识别有效提示的计算成本高昂。为解决这些问题，我们提出ZIUM――一种针对遗忘模型的零样本意图感知对抗攻击方法。该方法能够灵活定制目标攻击图像以反映攻击者意图，同时支持零样本对抗攻击，无需对先前已攻击的遗忘概念进行额外优化。在多种MU场景下的评估表明，ZIUM能成功根据用户意图提示定制内容，其攻击成功率优于现有方法。此外，其零样本对抗攻击特性显著缩短了对先前已攻击遗忘概念的攻击时间。
SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders,"Jiahui Geng, Qing Li","Unlearning methods for vision-language models (VLMs) have primarily adapted techniques from large language models (LLMs), relying on weight updates that demand extensive annotated forget sets. Moreover, these methods perform unlearning at a coarse granularity, often leading to excessive forgetting and reduced model utility. To address this issue, we introduce SAUCE, a novel method that leverages sparse autoencoders (SAEs) for fine-grained and selective concept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture high-dimensional, semantically rich sparse features. It then identifies the features most relevant to the target concept for unlearning. During inference, it selectively modifies these features to suppress specific concepts while preserving unrelated information. We evaluate SAUCE on two distinct VLMs, LLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks: concrete concept unlearning (objects and sports scenes) and abstract concept unlearning (emotions, colors, and materials), encompassing a total of 60 concepts. Extensive experiments demonstrate that SAUCE outperforms state-of-the-art methods by 18.04% in unlearning quality while maintaining comparable model utility. Furthermore, we investigate SAUCE's robustness against widely used adversarial attacks, its transferability across models, and its scalability in handling multiple simultaneous unlearning requests. Our findings establish SAUCE as an effective and scalable solution for selective concept unlearning in VLMs.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Geng_SAUCE_Selective_Concept_Unlearning_in_Vision-Language_Models_with_Sparse_Autoencoders_ICCV_2025_paper.html,0.9140625,SAUCE：基于稀疏自编码器的视觉语言模型选择性概念遗忘,视觉语言模型（VLM）的遗忘方法主要借鉴了大型语言模型（LLM）的技术，依赖于需要大量标注遗忘集的权重更新。此外，这些方法通常在粗粒度层面进行遗忘操作，往往导致过度遗忘并降低模型实用性。为解决这一问题，我们提出了SAUCE方法，这是一种利用稀疏自编码器（SAE）在VLM中实现细粒度选择性概念遗忘的新技术。简而言之，SAUCE首先训练SAE以捕获高维、语义丰富的稀疏特征，随后识别与目标遗忘概念最相关的特征。在推理过程中，该方法选择性地修改这些特征以抑制特定概念，同时保留无关信息。我们在两种不同的VLM（LLaVA-v1.5-7B和LLaMA-3.2-11B-Vision-Instruct）上对SAUCE进行了评估，涵盖两类任务：具体概念遗忘（物体与运动场景）和抽象概念遗忘（情绪、颜色与材质），共计60个概念。大量实验表明，SAUCE在保持相当模型实用性的同时，其遗忘质量比现有最优方法提升18.04%。此外，我们研究了SAUCE对常见对抗攻击的鲁棒性、跨模型迁移能力以及处理多重并行遗忘请求的可扩展性。我们的研究结果证实，SAUCE是VLM中实现选择性概念遗忘的有效且可扩展的解决方案。
DADet: Safeguarding Image Conditional Diffusion Models against Adversarial and Backdoor Attacks via Diffusion Anomaly Detection,"Hongwei Yu, Xinlong Ding, Jiawei Li, Jinlong Wang, Yudong Zhang, Rongquan Wang, Huimin Ma, Jiansheng Chen","While image conditional diffusion models demonstrate impressive generation capabilities, they exhibit high vulnerability when facing backdoor and adversarial attacks. In this paper, we define a scenario named diffusion anomaly where the generated results of a reverse process under attack deviate significantly from the normal ones. By analyzing the underlying formation mechanism of the diffusion anomaly, we reveal how perturbations are amplified during the reverse process and accumulated in the results. Based on the analysis, we reveal the phenomena of divergence and homogeneity, which cause the diffusion process to deviate significantly from the normal process and to decline in diversity. Leveraging these two phenomena, we propose a method named Diffusion Anomaly Detection (DADet) to effectively detect both backdoor and adversarial attacks. Extensive experiments demonstrate that our proposal achieves excellent defense performance against backdoor and adversarial attacks. Specifically, for the backdoor attack detection, our method achieves an F1 score of 99% on different datasets, including MS COCO and CIFAR-10. For the detection of adversarial samples, the F1 score exceeds 84% across three adversarial attacks and two different tasks, evaluated on the MS COCO and Places365 datasets, respectively.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Yu_DADet_Safeguarding_Image_Conditional_Diffusion_Models_against_Adversarial_and_Backdoor_ICCV_2025_paper.html,0.90625,DADet：通过扩散异常检测保护图像条件扩散模型免受对抗性与后门攻击,尽管图像条件扩散模型展现出令人印象深刻的生成能力，但当面临后门攻击和对抗性攻击时，它们表现出高度的脆弱性。本文定义了一种名为'扩散异常'的场景，即在遭受攻击时，逆向过程的生成结果与正常结果存在显著偏差。通过分析扩散异常的形成机制，我们揭示了扰动如何在逆向过程中被放大并累积至最终结果中。基于此分析，我们发现了导致扩散过程偏离正常轨迹并降低多样性的两种现象――发散性与同质性。利用这两种现象，我们提出了一种名为'扩散异常检测'的方法，可有效检测后门攻击和对抗性攻击。大量实验表明，我们的方法在防御后门攻击和对抗性攻击方面表现出色。具体而言，在后门攻击检测任务中，我们的方法在MS COCO和CIFAR-10等不同数据集上实现了99%的F1分数。在对抗性样本检测方面，通过在MS COCO和Places365数据集上分别评估三种对抗攻击和两种不同任务，其F1分数均超过84%。
TRCE: Towards Reliable Malicious Concept Erasure in Text-to-Image Diffusion Models,"Ruidong Chen, Honglin Guo, Lanjun Wang, Chenyu Zhang, Weizhi Nie, An-An Liu","Recent advances in text-to-image diffusion models enable photorealistic image generation, but they also risk producing malicious content, such as NSFW images. To mitigate risk, concept erasure methods are studied to facilitate the model to unlearn specific concepts. However, current studies struggle to fully erase malicious concepts implicitly embedded in prompts (e.g., metaphorical expressions or adversarial prompts) while preserving the model's normal generation capability. To address this challenge, our study proposes TRCE, using a two-stage concept erasure strategy to achieve an effective trade-off between reliable erasure and knowledge preservation. Firstly, TRCE starts by erasing the malicious semantics implicitly embedded in textual prompts. By identifying a critical mapping objective(i.e., the [EoT] embedding), we optimize the cross-attention layers to map malicious prompts to contextually similar prompts but with safe concepts. This step prevents the model from being overly influenced by malicious semantics during the denoising process. Following this, considering the deterministic properties of the sampling trajectory of the diffusion model, TRCE further steers the early denoising prediction toward the safe direction and away from the unsafe one through contrastive learning, thus further avoiding the generation of malicious content. Finally, we conduct comprehensive evaluations of TRCE on multiple malicious concept erasure benchmarks, and the results demonstrate its effectiveness in erasing malicious concepts while better preserving the model's original generation ability.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chen_TRCE_Towards_Reliable_Malicious_Concept_Erasure_in_Text-to-Image_Diffusion_Models_ICCV_2025_paper.html,0.87890625,TRCE：迈向文本到图像扩散模型中可靠恶意概念消除,文本到图像扩散模型的最新进展实现了逼真的图像生成，但也存在产生恶意内容（如NSFW图像）的风险。为降低风险，学界研究了概念擦除方法以使模型能够遗忘特定概念。然而，现有研究难以在保持模型正常生成能力的同时，完全擦除隐式嵌入提示中的恶意概念（例如隐喻表达或对抗性提示）。为解决这一挑战，本研究提出TRCE方法，采用两阶段概念擦除策略，在可靠擦除与知识保留之间实现有效平衡。首先，TRCE通过识别关键映射目标（即[EoT]嵌入向量），优化交叉注意力层将恶意提示映射至语境相似但包含安全概念的提示，从而擦除文本提示中隐式嵌入的恶意语义。这一步骤可防止模型在去噪过程中过度受恶意语义影响。随后，考虑到扩散模型采样轨迹的确定性特性，TRCE通过对比学习进一步引导早期去噪预测朝向安全方向并远离不安全方向，从而进一步避免恶意内容的生成。最后，我们在多个恶意概念擦除基准上对TRCE进行了全面评估，结果表明该方法在有效擦除恶意概念的同时，能更好地保留模型的原始生成能力。
SEAL: Semantic Aware Image Watermarking,"Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen","Generative models have rapidly evolved to generate realistic outputs. However, their synthetic outputs increasingly challenge the clear distinction between natural and AI-generated content, necessitating robust watermarking techniques to mark synthetic images. Watermarks are typically expected to preserve the integrity of the target image, withstand removal attempts, and prevent unauthorized insertion of the watermark pattern onto unrelated images. To address this need, recent methods embed persistent watermarks into images produced by diffusion models using the initial noise of the diffusion process. Yet, to do so, they either distort the distribution of generated images or require searching a large dictionary of candidate noise patterns for detection. In this paper, we propose a novel watermarking method that embeds semantic information about the generated image into the noise pattern, enabling a distortion-free watermark that can be verified without requiring a database of key patterns. Instead, the key pattern can be inferred from the semantic embedding of the image using locality-sensitive hashing. Furthermore, conditioning the watermark detection on the original image content improves its robustness against forgery attacks. To demonstrate that, we consider two largely overlooked attack strategies: (i) an attacker extracting the initial noise and generating a novel image with the same pattern; (ii) an attacker inserting an unrelated (potentially harmful) object into a watermarked image, while preserving the watermark. We empirically validate our method's increased robustness to these attacks. Taken together, our results suggest that content-aware watermarks can mitigate risks arising from image-generative models.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Arabi_SEAL_Semantic_Aware_Image_Watermarking_ICCV_2025_paper.html,0.87890625,SEAL：语义感知图像水印技术,生成模型迅速发展，已能产生高度逼真的输出结果。然而，其合成内容日益模糊了自然内容与人工智能生成内容之间的界限，这迫切需要采用鲁棒的水印技术来标记合成图像。理想的水印应能保持目标图像的完整性，抵御去除水印的尝试，并防止未经授权将水印图案插入无关图像。为满足这一需求，近期研究通过扩散过程的初始噪声，在扩散模型生成的图像中嵌入持久性水印。但现有方法要么会扭曲生成图像的分布，要么需要在检测时搜索庞大的候选噪声模式字典。本文提出一种新颖的水印方法，将生成图像的语义信息嵌入噪声模式中，从而实现无失真水印，且无需依赖密钥模式数据库即可验证。该方法通过局部敏感哈希从图像的语义嵌入中推断密钥模式。此外，将水印检测与原始图像内容相关联，可增强其抵御伪造攻击的能力。为验证这一点，我们研究了两种长期被忽视的攻击策略：（一）攻击者提取初始噪声并生成具有相同模式的新图像；（二）攻击者在保留水印的同时，将无关（可能有害）对象插入带水印的图像。我们通过实验验证了本方法对这些攻击具有更强的鲁棒性。综合而言，我们的研究结果表明，内容感知水印技术能够有效缓解图像生成模型带来的潜在风险。
"Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design","Yuhao Sun, Yihua Zhang, Gaowen Liu, Hongtao Xie, Sijia Liu","With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as ""challenging forgets"".",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Sun_Invisible_Watermarks_Visible_Gains_Steering_Machine_Unlearning_with_Bi-Level_Watermarking_ICCV_2025_paper.html,0.8671875,隐形水印，显性收益：通过双层水印设计引导机器遗忘,随着被遗忘权需求的日益增长，机器遗忘（MU）已成为提升信任与法规遵从性的关键工具，它能够从机器学习（ML）模型中移除敏感数据的影响。然而，大多数MU算法主要依赖训练内方法调整模型权重，对于数据层面调整可能为遗忘过程带来的益处探索有限。为填补这一空白，我们提出一种创新方法，通过策略性修改数据内容，利用数字水印技术促进MU。通过整合水印技术，我们建立了一种可控的遗忘机制，能够在保持模型与无关任务性能的同时，精确移除指定数据。我们首先研究了带水印数据对MU的影响，发现MU能有效泛化至带水印数据。在此基础上，我们提出一种遗忘友好的水印框架――Water4MU，以提升遗忘效能。Water4MU的核心是一个双层优化（BLO）框架：上层优化水印网络以最小化遗忘难度，下层则独立于水印进行模型训练。实验结果表明，Water4MU在图像分类和图像生成任务中均能有效实现MU。值得注意的是，在被称为'挑战性遗忘'的复杂MU场景中，其表现优于现有方法。
Unlearning the Noisy Correspondence Makes CLIP More Robust,"Haochen Han, Alex Jinpeng Wang, Peijun Ye, Fangming Liu","The data appetite for Vision-Language Models (VLMs) has continuously scaled up from the early millions to billions today, which faces an untenable trade-off with data quality and inevitably introduces Noisy Correspondence (NC) samples. Undoubtedly, such semantically unrelated data significantly impairs the performance of VLMs. Previous efforts mainly address this challenge by estimating refined alignment for more precise guidance. However, such resource-intensive pipelines that train VLMs from scratch struggle to meet realistic data demands. In this paper, we present a brand new perspective that seeks to directly eliminate the harmful effects of NC in pre-trained VLMs. Specifically, we propose NCU, a Noisy Correspondence Unlearning fine-tuning framework that efficiently enhances VLMs' robustness by forgetting learned noisy knowledge. The key to NCU is learning the hardest negative information, which can provide explicit unlearning direction for both false positives and false negatives. Such twin goals unlearning process can be formalized into one unified optimal transport objective for fast fine-tuning. We validate our approach with the prevailing CLIP model over various downstream tasks. Remarkably, NCU surpasses the robust pre-trained method on zero-shot transfer while with lower computational overhead. The code is available at https://github.com/hhc1997/NCU.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Han_Unlearning_the_Noisy_Correspondence_Makes_CLIP_More_Robust_ICCV_2025_paper.html,0.8359375,摒弃噪声对应关系，让CLIP模型更稳健,视觉语言模型（VLM）的数据需求已从早期的数百万规模持续攀升至如今的数十亿级别，这使其面临数据质量难以兼顾的困境，并不可避免地引入了噪声对应（NC）样本。毫无疑问，此类语义无关数据会严重损害VLMs的性能。先前的研究主要通过估算更精确的对齐关系来提供精细化指导以应对这一挑战。然而，这种需要从头训练VLMs的资源密集型流程难以满足实际数据需求。本文提出一种全新视角，旨在直接消除预训练VLMs中NC的有害影响。具体而言，我们提出NCU（噪声对应遗忘）微调框架，通过遗忘已习得的噪声知识来高效提升VLMs的鲁棒性。NCU的核心在于学习最困难的负样本信息，这能为假阳性与假阴性样本提供明确的遗忘方向。此类双重目标遗忘过程可形式化为统一的最优传输目标，从而实现快速微调。我们基于主流CLIP模型在多种下游任务中验证了本方法的有效性。值得注意的是，NCU在零样本迁移任务上超越了鲁棒预训练方法，同时具有更低的计算开销。代码已开源：https://github.com/hhc1997/NCU。
ROAR: Reducing Inversion Error in Generative Image Watermarking,"Hanyi Wang, Han Fang, Shi-Lin Wang, Ee-Chien Chang","Generative image watermarking enables the proactive detection and traceability of generated images. Among existing methods, inversion-based frameworks achieve highly conceal ed watermark embedding by injecting watermarks into the latent representation before the diffusion process. The robustness of this approach hinges on both the embedding mechanism and inversion accuracy. However, prior works have predominantly focused on optimizing the embedding process while overlooking inversion errors, which significantly affect extraction fidelity. In this paper, we address the challenge of inversion errors and propose ROAR, a dual-domain optimization-based framework designed to mitigate errors arising from two key sources: 1) Latent-domain errors, which accumulate across inversion steps due to inherent approximation assumptions. 2) Pixel-domain errors, which result from channel distortions such as JPEG compression. To tackle these issues, we introduce two novel components: A Regeneration-based Optimization (RO) mechanism, which incorporates an optimizable starting latent to minimize latent-domain errors; A Mixture of Experts (MoE)-based distortion-adaptive restoration (AR) network, which effectively recovers watermarked distributions from pixel-level distortions.Extensive experiments demonstrate that ROAR significantly reduces inversion errors and enhances watermark extraction robustness, thereby improving the reliability of generative image watermarking.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Wang_ROAR_Reducing_Inversion_Error_in_Generative_Image_Watermarking_ICCV_2025_paper.html,0.8359375,ROAR：降低生成式图像水印中的反演误差,生成式图像水隐写技术能够实现对生成图像的主动检测与溯源追踪。现有方法中，基于逆向重构的框架通过在扩散过程前将水印注入潜在表征，实现了高度隐蔽的水印嵌入。该方法的鲁棒性同时取决于嵌入机制与逆向重构精度。然而，先前研究主要聚焦于优化嵌入过程，却忽视了显著影响提取保真度的逆向重构误差。本文针对逆向重构误差的挑战，提出基于双域优化的ROAR框架，旨在缓解两个关键误差源：1）潜在域误差――由固有近似假设在逆向重构步骤中累积产生；2）像素域误差――源于JPEG压缩等通道失真。为解决这些问题，我们引入两个创新模块：基于再生优化的机制，通过可优化的初始潜在表征最小化潜在域误差；采用专家混合机制的失真自适应复原网络，有效从像素级失真中恢复水印分布。大量实验表明，ROAR框架显著降低了逆向重构误差，增强了水印提取的鲁棒性，从而提升了生成式图像水隐写技术的可靠性。
DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models,"Seunghoo Hong, Geonho Son, Juhun Lee, Simon S. Woo","Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive performance. In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Hong_DIA_The_Adversarial_Exposure_of_Deterministic_Inversion_in_Diffusion_Models_ICCV_2025_paper.html,0.796875,DIA：扩散模型中确定性反转的对抗性暴露,扩散模型已被证明是强大的表征学习工具，在多个领域展现出最先进的性能。除了加速采样外，DDIM还能将真实图像反演回其潜在编码。这一反演操作最直接的应用便是真实图像编辑――通过反演获得的潜在轨迹可在编辑图像的合成过程中加以利用。然而，这种实用工具也使得恶意用户能够更轻松地自由合成误导性信息或深度伪造内容，从而助长了不道德、滥用性内容以及侵犯隐私和版权内容的传播。尽管现有防御算法（如AdvDM和Photoguard）已被证明能干扰这些图像的扩散过程，但由于其目标与测试时的迭代去噪轨迹之间存在偏差，导致实际干扰效果有限。本研究提出了一种针对集成DDIM轨迹路径的攻击方法――DDIM反演攻击（DIA）。实验结果表明，该方法能实现有效干扰，在各种编辑方法中均超越了先前的防御技术。我们相信，该框架及研究成果能为产业界和学术界提供切实可行的防御方法，以应对人工智能的恶意使用。代码已开源：https://anonymous.4open.science/r/DIA-13419/。
TrustMark: Robust Watermarking and Watermark Removal for Arbitrary Resolution Images,"Tu Bui, Shruti Agarwal, John Collomosse","Imperceptible digital watermarking is important in copyright protection, misinformation prevention, and responsible generative AI. We propose TrustMark - a watermarking method that leverages a spatio-spectral loss function and a 1x1 convolution layer to enhance encoding quality. TrustMark is robust against both in-place and out-of-place perturbations while maintaining image quality above 43 dB. Additionally, we propose ReMark, a watermark removal method designed for re-watermarking, along with a simple yet effective algorithm that enables both TrustMark and ReMark to operate across arbitrary resolutions. Our methods achieve state-of-art performance on 3 benchmarks.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Bui_TrustMark_Robust_Watermarking_and_Watermark_Removal_for_Arbitrary_Resolution_Images_ICCV_2025_paper.html,0.796875,TrustMark：适用于任意分辨率图像的鲁棒水印嵌入与去除技术,不可感知数字水印技术在版权保护、虚假信息防范及负责任生成式人工智能领域具有重要意义。我们提出TrustMark――一种通过空间-频谱损失函数与1x1卷积层提升编码质量的水印方法。该方案在保持图像质量高于43dB的同时，对原位与非原位扰动均具有强鲁棒性。此外，我们设计了适用于重复水印场景的ReMark水印去除方法，并提出一种简洁高效的算法，使TrustMark与ReMark能够在任意分辨率下运行。我们的方法在三个基准测试中均达到了最先进的性能水平。
Moderating the Generalization of Score-based Generative Model,"Wan Jiang, He Wang, Xin Zhang, Dan Guo, Zhaoxin Fan, Yunfeng Diao, Richang Hong","Score-based Generative Models (SGMs) have demonstrated remarkable generalization capabilities, e.g. generating unseen, but natural data. However, the greater the generalization power, the more likely the unintended generalization, and the more dangerous the abuse. Despite these concerns, research on unlearning SGMs has not been explored. To fill this gap, we first examine the current `gold standard' in Machine Unlearning (MU), i.e., re-training the model after removing the undesirable training data, and find it does not work in SGMs. Further analysis of score functions reveals that the MU 'gold standard' does not alter the original score function, which explains its ineffectiveness. Building on this insight, we propose the first Moderated Score-based Generative Model (MSGM), which introduces a novel score adjustment strategy that redirects the score function away from undesirable data during the continuous-time stochastic differential equation process. Albeit designed for SGMs, MSGM is a general and flexible MU framework compatible with diverse diffusion architectures, training strategies and downstream tasks. Code is available at https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Jiang_Moderating_the_Generalization_of_Score-based_Generative_Model_ICCV_2025_paper.html,0.796875,基于分数的生成模型的泛化能力调节,基于分数的生成模型（SGMs）已展现出卓越的泛化能力，例如生成未见但自然的数据。然而，泛化能力越强，发生非预期泛化的可能性就越大，其滥用风险也越高。尽管存在这些担忧，针对SGMs的遗忘学习研究尚未得到探索。为填补这一空白，我们首先检验了机器学习遗忘领域当前的'黄金标准'――即在移除不良训练数据后重新训练模型，并发现该方法在SGMs中并不适用。对评分函数的进一步分析表明，该遗忘学习'黄金标准'并未改变原始评分函数，这解释了其失效的原因。基于这一发现，我们提出了首个基于分数的调节生成模型（MSGM），该模型引入了一种新颖的评分调整策略，能在连续时间随机微分方程过程中将评分函数从不良数据方向转移。尽管专为SGMs设计，MSGM仍是一个通用且灵活的遗忘学习框架，可兼容多种扩散架构、训练策略及下游任务。代码已发布于https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model。
AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs,"Sanjoy Chowdhury, Sayan Nag, Subhrajyoti Dasgupta, Yaoting Wang, Mohamed Elhoseiny, Ruohan Gao, Dinesh Manocha","With the rapid advancement of Multi-modal Large Language Models (MLLMs), several diagnostic benchmarks have recently been developed to assess these models' multimodal reasoning proficiency. However, these benchmarks are restricted to assessing primarily the visual aspect and do not examine the holistic audio-visual (AV) understanding. Moreover, currently, there are no benchmarks that investigate the capabilities of AVLLMs to calibrate their responses when presented with perturbed inputs. To this end, we introduce Audio-Visual Trustworthiness assessment Benchmark (AVTrustBench), comprising 600K samples spanning over 9 meticulously crafted tasks, evaluating the capabilities of AVLLMs across three distinct dimensions: Adversarial Attack, Compositional Reasoning, and Modality-specific Dependency. Using our benchmark, we extensively evaluate 16 state-of-the-art AVLLMs. The findings reveal that the majority of existing models fall significantly short of achieving human like comprehension, offering valuable insights for future research directions. To alleviate the limitations in the existing approaches, we further propose a robust, model agnostic calibrated audio-visual preference optimization based training strategy CAVPref, obtaining a gain up to 30.19% across all 9 tasks.",2025,https://openaccess.thecvf.com/content/ICCV2025/html/Chowdhury_AVTrustBench_Assessing_and_Enhancing_Reliability_and_Robustness_in_Audio-Visual_LLMs_ICCV_2025_paper.html,0.796875,AVTrustBench：评估与增强视听大语言模型的可靠性与鲁棒性,随着多模态大语言模型（MLLMs）的快速发展，近期已出现多个诊断性基准测试用于评估这些模型的多模态推理能力。然而，现有基准主要局限于视觉维度的评估，未能全面考察视听（AV）综合理解能力。此外，目前尚无基准测试能够探究视听大语言模型（AVLLMs）在面对干扰输入时对其响应进行校准的能力。为此，我们提出视听可信度评估基准（AVTrustBench），该基准包含60万个样本，涵盖9项精心设计的任务，从三个不同维度评估AVLLMs的能力：对抗性攻击、组合推理和模态特定依赖性。基于此基准，我们对16个前沿AVLLMs进行了全面评估。研究结果表明，现有大多数模型远未达到类人理解水平，这为未来研究方向提供了重要启示。为缓解现有方法的局限性，我们进一步提出一种鲁棒的、模型无关的校准视听偏好优化训练策略CAVPref，该策略在全部9项任务中实现了最高30.19%的性能提升。
