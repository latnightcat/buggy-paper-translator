#  项目日记

​	虽然在这个项目之前我并未接触过python语言，也没有使用过pycharm的经验，于是我在这几天简单学习了一下相关语法和pycharm的使用之后，也解决了大部分问题。

​	首先再api调用上似乎也出现了问题，原代码似乎是想使用openai官方的api来调用大模型，但参数却是hugging face的token和hugging face网站的url。

​	对于过气的api借口，我改用了huggingface_hub.InferenceClient，导入了huggingface_hub这个包，因此删去了requests这个包，因为huggingface_hub这个包内就封装了requests，实现了与Hugging Face API进行交互所需的所有细节。

​	然后这些包中qtdm出现了拼写错误，正确应该是tqdm。

​	对于选择的模型，我选择的是国产模型deepseek-ai/DeepSeek-V3.2，并根据要求显示指定策略为：fastest。

​	对于模型的一些参数，原代码出现一些错误，比如temperature值设置过高，这个参数控制模型输出的随机性，数值太高会导致模型输出偏离事实，尤其对于翻译工作来说，准确才是最大的需求，因此我将其改为0.3，保证了翻译的准确性。其次对于max_tokens,这个参数限制模型在一次调用中最多能生成的词元数量，值太小，翻译还未完成就结束了，对于较长文本难以实现完全翻译，因此我将其改为512，以确保整个文本都能被翻译。

​	同时原始代码将Hugging Face的token设置为硬编码形式，使得代码的可移植性变差，同时还有泄露token的风险，我按照要求将token值的获取方式改为了通过os.environ.get函数从系统环境变量中读取，虽然使得用户需要在运行代码前手动配置环境变量，但大大增加了代码的可移植性，并保护了token的隐私性，防止泄露。

​	然后对于prompt效果较差的问题，我通过结合ai，编写了一个更专业、更适合学术论文的prompt，使得翻译的结果更符合需求。

​	同时也增加try-except和tenacity重试机制，最多重试5次，防止网络波动等问题导致程序运行失败。

​	也根据要求引入了tqdm显示进度，让用户直观看到翻译的进度。

​	也加入了断电续传功能，准确来说，该脚本在执行时，会先判断是否已经存在result.csv文件。如果有，就

将该表格的数据合并到存储原始icvv025.csv数据的df中，再对df中其它没翻译的地方继续翻译，因此，除了能够实现断电续传外，对于某一列翻译错误的情况，脚本也能对这一行进行翻译，而不是全文重新翻译，大大增加了实用性。

​	同时我发现，原代码的time.sleep()参数的值也设置的不合理，代码中该函数的功能每翻译一行就等待一段时间，避免像api服务器发送过多请求，但1000的值代表每次等待1000秒，那翻译整个完估计好几小时了，因此改成3，每次等待3秒即可。

​	还有就是原代码中文件写入方式（下列代码），w模式会在每次写入时覆盖上一次写入结果，导致最后就剩最后一个文本的翻译了，可以改成a（追加）模式，但我通过学习发现使用DataFrame来保存整个结果效果更好，更容易对数据进行管理。

```
  with open("result.csv", "w", encoding="utf-8") as f:
            f.write(f"{row['id']},{cn_abstract}\n")
```

​	以上便是我的项目完成的过程。